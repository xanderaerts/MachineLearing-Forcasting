{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: Get the imports & import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal = pd.read_csv(\"nepal_train.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We starten met een aantal zaken te bekijken over de data, om zo te weten te komen waarmee we te maken hebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal['foundation_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Clean the data & Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal.isna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We beginnen met het weggooien van het building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nepal['building_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het viel mij op dat er een aantal kolommen waren met heel weinig 'true' values. Hierdoor heb ik besloten deze weg te laten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kolom in nepal.columns:\n",
    "    print(nepal[kolom].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kolom in nepal.columns:\n",
    "        unieke_waarden = nepal[kolom].unique()\n",
    "        if set(unieke_waarden).issubset({0, 1}):\n",
    "            aantal_eenen = (nepal[kolom] == 1).sum()\n",
    "            if aantal_eenen < 500:\n",
    "                nepal.drop(columns=kolom,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken nu het probleem binary en passen daarna one-hot-encoding toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal['damage_grade'] = nepal['damage_grade'].replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal = pd.get_dummies(nepal,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zonderen eerst ons target af om daarna een train/test split te maken met een test data van 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nepal['damage_grade']\n",
    "y = y.replace({1:0,3:1})\n",
    "del nepal['damage_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(nepal,y,test_size=0.1,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken een randomforest aan en trainen dit met de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_preds = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score\n",
    "cm = confusion_matrix(y_test,RF_preds)\n",
    "CM_plot = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                 display_labels=RF.classes_)\n",
    "CM_plot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = accuracy_score(y_test,RF_preds)\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat ons model nu een score haalt van +/- 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(RF, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print( np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien hier een gemiddelde ROC-AUC score van +/- 0.78 wat niet perse goed of heel slecht hoeft te zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob = RF.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# ROC curve berekenen\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# ROC-curve plotten\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TPrate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Precision-Recall-curve plotten\n",
    "plt.plot(recall, precision, label='PR curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Parametertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passen parameter tuning toe om zo het best mogelijke model te vinden en dit te kunnen gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    X_train, X_test, y_train,y_test = train_test_split(nepal,y,test_size=0.90,random_state=101)\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],       \n",
    "        'max_depth': [None, 10, 20, 30],    \n",
    "        'min_samples_split': [2, 5, 10],      \n",
    "        'min_samples_leaf': [1, 2, 4],        \n",
    "        }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=RF, param_grid=param_grid, cv=5, scoring='roc_auc',n_jobs=10,verbose=2)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_RF = RandomForestClassifier(**grid_search.best_params_, random_state=101)\n",
    "\n",
    "    best_RF.fit(X_train, y_train)\n",
    "\n",
    "    best_RF_preds = best_RF.predict(X_test)\n",
    "\n",
    "    best_accuracy = accuracy_score(y_test, best_RF_preds)\n",
    "    print(\"Accuracy:\", best_accuracy)\n",
    "\n",
    "    cv_scores_best = cross_val_score(best_RF, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print(\"Gem ROC-AUC \", np.mean(cv_scores_best))\n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_prob = RF.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  \n",
    "    plt.xlabel('FP rate')\n",
    "    plt.ylabel('TP rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "    plt.plot(recall, precision, label='PR curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook hier zien we dat het model een accuracy haalt van +/- 78%\n",
    "Ook bij de gemiddelde Roc-AUC score zien we geen grote verschillen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Use test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepalTest= pd.read_csv(\"nepal_test.csv\",sep=\";\")\n",
    "nepalTest['building_id']\n",
    "\n",
    "for kolom in nepalTest.columns:\n",
    "        unieke_waarden = nepalTest[kolom].unique()\n",
    "        if set(unieke_waarden).issubset({0, 1}):\n",
    "            aantal_eenen = (nepalTest[kolom] == 1).sum()\n",
    "            if aantal_eenen < 500:\n",
    "                nepalTest.drop(columns=kolom,inplace=True)\n",
    "\n",
    "\n",
    "nepalTest['damage_grade'] = nepalTest['damage_grade'].replace(2,1)\n",
    "\n",
    "nepalTest = pd.get_dummies(nepalTest,dtype=int)\n",
    "\n",
    "\n",
    "y = nepalTest['damage_grade']\n",
    "y = y.replace({1:0,3:1})\n",
    "del nepalTest['damage_grade']\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(nepalTest,y,test_size=0.1,random_state=101)\n",
    "\n",
    "\n",
    "RF.fit(X_train,y_train)\n",
    "\n",
    "RF_preds = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = accuracy_score(y_test,RF_preds)\n",
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(RF, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print( np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat ons model zo goed als de zelfde score haalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gebruik predict_proba om de kansvoorspelling voor de klasse 1 (positieve klasse) te krijgen\n",
    "y_prob = RF.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# ROC curve berekenen\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# ROC-curve plotten\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonale lijn voor willekeurige voorspellingen\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve berekenen\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Precision-Recall-curve plotten\n",
    "plt.plot(recall, precision, label='PR curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de curves zijn ook geen grote merkwaardige verschillen te vinden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
